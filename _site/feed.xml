<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-12-23T16:19:14-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lara J. Martin</title><subtitle>Homepage to Dr. Lara J. Martin</subtitle><entry><title type="html">ChatGPT will not replace writers…unless we let it</title><link href="http://localhost:4000/2023/07/01/writers-strike.html" rel="alternate" type="text/html" title="ChatGPT will not replace writers…unless we let it" /><published>2023-07-01T01:00:00-04:00</published><updated>2023-07-01T01:00:00-04:00</updated><id>http://localhost:4000/2023/07/01/writers-strike</id><content type="html" xml:base="http://localhost:4000/2023/07/01/writers-strike.html"><![CDATA[<p>It looks like the Writers Guild (the union for people who write movies and TV shows) is striking again, this time <a href="https://www.nytimes.com/2023/05/10/arts/television/writers-strike-artificial-intelligence.html">because</a> <a href="https://www.polygon.com/23742770/ai-writers-strike-chat-gpt-explained">of</a> <a href="https://www.npr.org/2023/05/20/1177366800/striking-movie-and-tv-writers-worry-that-they-will-be-replaced-by-ai">ChatGPT</a>.</p>

<p>Hi, I’m Lara. I wrote <a href="https://repository.gatech.edu/handle/1853/64643">my dissertation on using AI to automatically generate stories</a>, so I have some opinions about using AI to write stories.
I also spoke to my wife, <a href="https://dekent.github.io/">Cassie Kent</a>—who does human-robot interaction—about this, and this blog post is an amalgamation of our thoughts.</p>

<p>So what do we think?
Namely: stop replacing writers with AI.</p>

<p>I’m not surprised this is happening and I’m 100% on the writers’ side.
But I wanted to make this blog post to hash out some of the nuances of using ChatGPT (or other large language models; I’ll use “ChatGPT” as a shorthand) for creative tasks such as writing. (If you’d like a quick primer into what ChatGPT or a large language model is, check out <a href="https://laramartin.net/2022/06/21/lamda-is-not-sentient.html">my blog post from last year</a>.)</p>

<p>But I digress.</p>

<p>Beyond the issues of equal pay and ethical treatment of writers, movie studios are missing the big picture. Movie generation using <em>AI only</em> to write stories is not going to hold up long term.
ChatGPT cannot replace writers because they’re not going to be able to tell the types of stories that people want to hear.
That is, <strong>people have stories to tell and AI lacks the lived experiences to tell those stories</strong>. (If you come away with anything from reading this post, this is it.)</p>

<h2 id="but-what-if-people-only-want-moviesart-for-entertainment-and-not-communication">But what if people only want movies/art for entertainment and not communication?</h2>
<p>Even if you think of AI-created art as entertainment only and not telling some personal story, it might be able to crank out a few okay stories until people will begin to realize they’re getting the same story over and over again and things would get really boring.</p>

<p>You see, ChatGPT works off of likelihoods, and so it’s like it’s playing tug of war between being either coherent or unique. If we pick out really likely words, it’s going to want to spit out the most likely (i.e., boring) story possible. On the other hand, if you pick out really unlikely words, it’s going to be unique but unintelligible or at least forget what it’s talking about over time. So it’s less like “haha that was so interesting and clever” and more like “haha the AI really has no idea what it’s talking about”.</p>

<p>Now, even though Hollywood thinks we want the same movies over and over again, if we <em>actually</em> got the same movies, people would stop paying for movies.
They will start to tune things out or get bored.
And over time, directors and actors will refuse to work with those scripts.</p>

<p>Furthermore, culture changes over time and ChatGPT struggles with the intricacies of culture already. It would not be able to reflect the diverse experiences that we have as humans.
Things that we thought 20/30 years ago are quickly becoming outdated (or honestly even 3 years ago - e.g., COVID).
Humor is something that AI struggles with already and it changes very quickly and is extremely culture-specific.</p>

<p>Also, current entertainment already has a huge problem with representation. Older shows and movies have a much bigger problem. We can’t expect AI to fix this without human writers representing these perspectives and moving things forward.</p>

<h2 id="so-should-we-just-stop-using-chatgpt-for-writing-scripts">So should we just stop using ChatGPT for writing scripts?</h2>
<p>No, I don’t think so, and I’m not just saying that because I’m working on these tools.</p>

<p>I believe that <strong>AI and people collaborating</strong> is where we find our strengths. Computers are great at processing a large amount of data and humans are great at making connections between ideas. Also, what is AI if not to either 1) help people or 2) get a glimpse into understanding how cognition works? (“People” here can also be animals.)</p>

<p>Plus, having these tools available means that people with less writing experience could write a decent script or that experienced writers could write scripts in a shorter period of time.</p>

<h2 id="some-upcoming-issues-with-using-ai-in-writing">Some upcoming issues with using AI in writing</h2>
<p>That said, because these tools are proprietary, there is a big concern about access. Will only high-end studios have access to these tools that can help their writers? Will studios continue to be conservative with their money and figure: “I’m already paying a person to write, I don’t want to pay for ChatGPT too”?</p>

<p>I don’t know enough about the writing process but I can certainly see a future where AI will decrease the amount of writers for a particular movie or show—not eliminate them completely. 
This presents risks to covering the diversity of peoples’ experiences that we should be aware of and try to mitigate.
(However, with fewer people on one team you might be able to have more projects going on at the same time as long as other parts of the movie-making process also become cheaper.)</p>

<p>I don’t have the answers to these questions, and we don’t exactly know how things are going to turn out, but it’s good to be aware of what could happen so that we can be prepared.</p>

<h2 id="parting-thoughts">Parting Thoughts</h2>
<p>The people who make these technologies should take responsibility for the good and the bad that they put out into the world.</p>

<p>Even though the automated generation of stories might not be as good as human-written stories, capitalism might not recognize this (but it should because using only AI won’t be profitable long-term) and that’s why writers strikes are important so that we can keep pushing back.</p>

<p>If you’d like to hear more about AI’s involvement in art, check out <a href="https://korymathewson.com/art/">this blog post by my friend Kory Mathewson</a> and <a href="https://newart.press/p/art-has-always-been-artifical">this article by Louis Anslow</a>.</p>]]></content><author><name></name></author><category term="Language Models" /><category term="Neural Networks" /><category term="ChatGPT" /><category term="AI and Society" /><summary type="html"><![CDATA[What you should and shouldn't be concerned about when it comes to your writing job.]]></summary></entry><entry><title type="html">No, LaMDA Isn’t Sentient</title><link href="http://localhost:4000/2022/06/21/lamda-is-not-sentient.html" rel="alternate" type="text/html" title="No, LaMDA Isn’t Sentient" /><published>2022-06-21T01:00:00-04:00</published><updated>2022-06-21T01:00:00-04:00</updated><id>http://localhost:4000/2022/06/21/lamda-is-not-sentient</id><content type="html" xml:base="http://localhost:4000/2022/06/21/lamda-is-not-sentient.html"><![CDATA[<p>You might have come across <a href="https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917">an article</a> being shared by some of your friends and family on Facebook recently.
In it, the author claims to have an “interview” with an AI called LaMDA in which the AI “claims to be sentient.”
For context, I am researcher who works in artificial intelligence, natural language generation (NLG), and cognitive modeling who has directly worked with LaMDA (paper published soon), among other similar language models (LMs). And I can say with 100% certainty that LaMDA is not sentient.</p>

<p><strong>Can it say things that make it seem like it has metacognition—the ability to think about thinking?</strong> Sure.</p>

<p><strong>Does it have metacognition?</strong> No. <del>There isn’t even a mechanism in place for it to reflect on its own behavior, let alone what it “thinks”.</del> Correction: There is a <a href="https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html">mechanism in place for it to rate its own output/behavior</a>, but I still wouldn’t call this “reflection”.</p>

<p><strong>Is this a super cool example of using LMs to generate text?</strong> Absolutely. I always think it’s super cool to see what people do with these models.</p>

<p><strong>Are we close to achieving <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">AGI (artificial general intelligence)</a>?</strong> No. Even if it does happen, it probably won’t be in our lifetimes. We have not made the relevant advances to get there. We have a hard enough time getting AI to do <em>more than one task</em> let alone be capable to do everything a dog, dolphin, or donkey can do.</p>

<h2 id="language-models">Language Models</h2>

<p>If you’re unfamiliar with how language models work, it’s a bit like this: You take a ridiculous amount of text (read: the internet) and you run it through a “neural network”. This network is literally just doing a bunch of multi-dimensional multiplications of numbers (called matrix multiplications). You run the data through multiple times, and it eventually abstracts away patterns in the data. This process is called training.</p>

<p><img src="/assets/img/nn.webp" alt="Deep neural network diagram by IBM." />
<em>Deep neural network diagram by IBM. Image from <a href="https://www.ibm.com/cloud/learn/neural-networks">What are Neural Networks?</a>. The “deep” part refers to the hidden layers in the middle.</em></p>

<p>Then when you want to use it, you load up the trained language model into your computer’s memory and give it a prompt. The prompt might be a single word, or it can be a whole question like in the article’s interview. Based on this prompt/input, the LM will give you the most likely text to come next based on the text it trained on. This is generally called running or testing the model.</p>

<center>&mdash;</center>

<p>Working with these models is a bit of an art.</p>

<p><a href="https://openreview.net/pdf?id=rygGQyrFvH">If you’re not careful</a> when you run the model, the language model might give you the most probable word or phrase each time, and you’ll end up getting some very boring text. (Lots of “I don’t know”s.)
But if you go too far the other way—picking the least probable next word or phrase, the language model spits out nonsense!</p>

<p>Training can be tricky to get working right too. 
If you don’t train the network long enough, it doesn’t pick up enough information about the data to really do anything with it. It might not even learn how to spell.</p>

<p>And if you train for too long, the model just memorizes all the data you give it, which might generate some impressive-sounding text because it’s basically copying what a human said verbatim. However, when you give it a prompt is has never seen before, it flounders and gives you gibberish.</p>

<p>With really big language models (like LaMDA), they can <a href="https://arxiv.org/abs/2202.07646">end up memorizing data</a> even when they’re <a href="https://arxiv.org/abs/2205.10770">not trained for too long</a>.</p>

<p>Even if a neural network ends up being really good at one task, it is incredibly difficult to train the network on a new task and <a href="https://link.springer.com/chapter/10.1007/978-3-030-58598-3_28">not</a> <a href="https://www.pnas.org/doi/10.1073/pnas.1611835114">let</a> <a href="https://www.sciencedirect.com/science/article/pii/S1364661317300736">it</a> <a href="https://proceedings.neurips.cc/paper/2017/hash/f708f064faaf32a43e4d3c784e6af9ea-Abstract.html">forget</a> what it learned in the first task. This is called <em>catastrophic forgetting</em>, which probably isn’t that important to know, but I wanted to share since it’s one of my favorite phrases in AI.</p>

<p>All this is to say that neural networks are (1) highly-engineered tools and (2) <em>very loosely inspired</em> by the architecture of the brain, at best.</p>

<h2 id="but-what-is-lamda-specifically">But what is LaMDA, specifically?</h2>

<p>The “really big language models” I’m talking about are called transformers. <a href="https://towardsdatascience.com/transformer-neural-network-step-by-step-breakdown-of-the-beast-b3e096dc857f">Transformer language models</a> such as GPT and BERT (and their “relatives”) have pushed the state-of-the-art of natural language processing (and probably many other computing fields). These neural networks have a special component called <a href="https://www.kaggle.com/code/residentmario/transformer-architecture-self-attention/notebook">“attention”</a>, which basically tells the LM “when you look at a word, pay attention to its context too”.</p>

<p><img src="/assets/img/PLMfamily.jpg" alt="Family of Pretrained Language Models as of a year ago by Xiaozhi Wang and Zhengyan Zhang." />
<em>Family of Pretrained Language Models as of a year ago by Xiaozhi Wang and Zhengyan Zhang. A lot more have come out in the past year. Image from the <a href="https://github.com/thunlp/PLMpapers">PLM papers repo</a>.</em></p>

<p><a href="https://blog.google/technology/ai/lamda/">LaMDA</a> stands for “Language Models for Dialog Applications”. It is a transformer-based language model created by <a href="https://arxiv.org/abs/2201.08239">researchers at Google</a>* that was specifically created for dialog (think chatbot).</p>

<p>*It might be worth noting that Blake Lemoine (the author of the original “interview” article) is not one of the listed authors.</p>

<h2 id="parting-thoughts">Parting Thoughts</h2>

<ul>
  <li>
    <p><strong>Will AI ever be sentient?</strong> It’s not for me to say whether or not AI will ever be sentient, especially as we go deeper into <a href="https://en.wikipedia.org/wiki/Biological_computing">biological computing</a>. I can’t predict the future, but like I said, even if AI were to ever be sentient some day, it will not be during our lifetimes.</p>
  </li>
  <li>
    <p><strong>Should we even be caring about whether or not AI is sentient?</strong> I’d argue no. Just because something isn’t sentient doesn’t mean you shouldn’t care about it. I respect my Roomba’s existence as much as I respect my plants’ or my notebook’s existence. They are all important parts of my environment even if my plants require more care because they are alive.</p>
  </li>
  <li>
    <p><strong>Want to check out more fun interviews with large language models?</strong> Janelle Shane interviews a squirrel, a vacuum, and other things via GPT-3: <a href="https://www.aiweirdness.com/interview-with-a-squirrel/">https://www.aiweirdness.com/interview-with-a-squirrel/</a></p>
  </li>
</ul>]]></content><author><name></name></author><category term="Language Models" /><category term="Neural Networks" /><category term="Artificial General Intelligence" /><summary type="html"><![CDATA[I explain at a high level why LaMDA can't be sentient.]]></summary></entry></feed>